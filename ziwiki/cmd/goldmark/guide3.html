<h1 class='text-3xl font-black text-slate-900 tracking-tight text-center dark:text-slate-200  pb-6 my-3'>Contents</h1>
<ul class='list-disc  text-base text-slate-800 dark:text-slate-300  pl-4'>
<li>
<a href="#font-color--darkbluecentercenterfont">归一化以及正则化</a><ul class='list-disc  text-base text-slate-800 dark:text-slate-300  pl-4'>
<li>
<ul class='list-disc  text-base text-slate-800 dark:text-slate-300  pl-4'>
<li>
<a href="#batchnorm">batchnorm</a></li>
<li>
<a href="#layer-norm">layer norm</a></li>
<li>
<a href="#heading">正则化</a></li>
</ul>
</li>
<li>
<a href="#2---l2-regularization">2 - L2 Regularization</a></li>
</ul>
</li>
</ul>
<h1 class='text-3xl font-black text-slate-900 tracking-tight text-center dark:text-slate-200  pb-6 my-3' id="font-color--darkbluecentercenterfont"><!-- raw HTML omitted --><!-- raw HTML omitted -->归一化以及正则化<!-- raw HTML omitted --><!-- raw HTML omitted --></h1>
<ol class='list-decimal text-base text-slate-800 dark:text-slate-300  pl-4'>
<li>归一化的必要性:
考虑梯度下降优化，未归一化两个维度更新幅度不一样，导致收敛慢</li>
</ol>
<h3 class='text-xl  font-extrabold	text-slate-900 tracking-tight dark:text-slate-200  py-1.5' id="batchnorm">batchnorm</h3>
<!-- raw HTML omitted -->
<div class="relative z-10 mx-2 my-6 col-span-3 dark:bg-slate-800 bg-white font-semibold rounded-md shadow-lg  ring-1 ring-black/10 dark:ring-1 dark:ring-white/10 dark:ring-inset">
				<div class="relative flex text-slate-400 text-sm leading-6">
						<div class="mt-2 flex-none dark:text-sky-300 text-slate-800 border-t border-b border-t-transparent border-b-slate-600 dark:border-b-sky-300 px-4 py-1 flex items-center">
								language:$2
						</div>
						<div class="flex-auto flex pt-2 rounded-tr-xl overflow-hidden">
								<div class="flex-auto -mr-px bg-slate-100 dark:bg-slate-700/50 border border-slate-500/30 rounded-tl">
								</div>
						</div>
						<div class="absolute top-2 right-0 h-8 flex items-center pr-4">
								<div class="relative flex -mr-2">
								<span class="copycontent"/>
								</div>
						</div>
				</div>
	<div class="highlight p-4  text-sm  overflow-x-auto  text-slate-800 dark:text-slate-200
							scrollbar-thin  scrollbar-thumb-rounded-md scrollbar-track-rounded-md
							scrollbar-track-white scrollbar-thumb-slate-200 
							dark:scrollbar-track-slate-800 dark:scrollbar-thumb-slate-500
	"><pre><code><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]">1</span><span class="cl"><span class="dark:text-[#89DDFF] text-[#a90d91]">import</span> <span class="dark:text-[#FFCB6B] text-[#000000]">numpy</span> <span class="dark:text-[#BB80B3] text-[#a90d91]">as</span> <span class="dark:text-[#FFCB6B] text-[#000000]">np</span>
</span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]">2</span><span class="cl"><span class="dark:text-[#EEFFFF] text-[#000000]">x</span><span class="dark:text-[#89DDFF] text-[#000000]">=</span><span class="dark:text-[#EEFFFF] text-[#000000]">np</span><span class="dark:text-[#89DDFF] text-[#000000]">.</span><span class="dark:text-[#EEFFFF] text-[#000000]">random</span><span class="dark:text-[#89DDFF] text-[#000000]">.</span><span class="dark:text-[#EEFFFF] text-[#000000]">random</span><span class="dark:text-[#89DDFF]">([</span><span class="dark:text-[#F78C6C] text-[#1c01ce]">10</span><span class="dark:text-[#89DDFF]">,</span><span class="dark:text-[#F78C6C] text-[#1c01ce]">4</span><span class="dark:text-[#89DDFF]">])</span>
</span></span></code></pre></div></div><div class="relative z-10 mx-2 my-6 col-span-3 dark:bg-slate-800 bg-white font-semibold rounded-md shadow-lg  ring-1 ring-black/10 dark:ring-1 dark:ring-white/10 dark:ring-inset">
				<div class="relative flex text-slate-400 text-sm leading-6">
						<div class="mt-2 flex-none dark:text-sky-300 text-slate-800 border-t border-b border-t-transparent border-b-slate-600 dark:border-b-sky-300 px-4 py-1 flex items-center">
								language:$2
						</div>
						<div class="flex-auto flex pt-2 rounded-tr-xl overflow-hidden">
								<div class="flex-auto -mr-px bg-slate-100 dark:bg-slate-700/50 border border-slate-500/30 rounded-tl">
								</div>
						</div>
						<div class="absolute top-2 right-0 h-8 flex items-center pr-4">
								<div class="relative flex -mr-2">
								<span class="copycontent"/>
								</div>
						</div>
				</div>
	<div class="highlight p-4  text-sm  overflow-x-auto  text-slate-800 dark:text-slate-200
							scrollbar-thin  scrollbar-thumb-rounded-md scrollbar-track-rounded-md
							scrollbar-track-white scrollbar-thumb-slate-200 
							dark:scrollbar-track-slate-800 dark:scrollbar-thumb-slate-500
	"><pre><code><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]"> 1</span><span class="cl"><span class="dark:text-[#BB80B3] text-[#a90d91]">def</span> <span class="dark:text-[#82AAFF] text-[#000000]">batchnorm</span><span class="dark:text-[#89DDFF]">(</span><span class="dark:text-[#EEFFFF] text-[#000000]">x</span><span class="dark:text-[#89DDFF]">):</span>
</span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]"> 2</span><span class="cl">    <span class="dark:text-[#C3E88D] text-[#c41a16]">&#34;&#34;&#34;
</span></span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]"> 3</span><span class="cl"><span class="dark:text-[#C3E88D] text-[#c41a16]">    x.shape:b,dim
</span></span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]"> 4</span><span class="cl"><span class="dark:text-[#C3E88D] text-[#c41a16]">    &#34;&#34;&#34;</span>
</span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]"> 5</span><span class="cl">    <span class="dark:text-[#EEFFFF] text-[#000000]">mu</span><span class="dark:text-[#89DDFF] text-[#000000]">=</span><span class="dark:text-[#EEFFFF] text-[#000000]">np</span><span class="dark:text-[#89DDFF] text-[#000000]">.</span><span class="dark:text-[#EEFFFF] text-[#000000]">sum</span><span class="dark:text-[#89DDFF]">(</span><span class="dark:text-[#EEFFFF] text-[#000000]">x</span><span class="dark:text-[#89DDFF]">,</span><span class="dark:text-[#F78C6C] text-[#1c01ce]">0</span><span class="dark:text-[#89DDFF]">)</span><span class="dark:text-[#89DDFF] text-[#000000]">/</span><span class="dark:text-[#EEFFFF] text-[#000000]">x</span><span class="dark:text-[#89DDFF] text-[#000000]">.</span><span class="dark:text-[#EEFFFF] text-[#000000]">shape</span><span class="dark:text-[#89DDFF]">[</span><span class="dark:text-[#F78C6C] text-[#1c01ce]">0</span><span class="dark:text-[#89DDFF]">]</span>
</span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]"> 6</span><span class="cl">
</span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]"> 7</span><span class="cl">    <span class="dark:text-[#EEFFFF] text-[#000000]">var</span><span class="dark:text-[#89DDFF] text-[#000000]">=</span><span class="dark:text-[#F78C6C] text-[#1c01ce]">0</span>
</span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]"> 8</span><span class="cl">    <span class="dark:text-[#BB80B3] text-[#a90d91]">for</span> <span class="dark:text-[#EEFFFF] text-[#000000]">i</span> <span class="ow">in</span> <span class="dark:text-[#82AAFF] text-[#a90d91]">range</span><span class="dark:text-[#89DDFF]">(</span><span class="dark:text-[#82AAFF] text-[#a90d91]">len</span><span class="dark:text-[#89DDFF]">(</span><span class="dark:text-[#EEFFFF] text-[#000000]">x</span><span class="dark:text-[#89DDFF]">)):</span>
</span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]"> 9</span><span class="cl">        <span class="dark:text-[#EEFFFF] text-[#000000]">var</span><span class="dark:text-[#89DDFF] text-[#000000]">+=</span><span class="dark:text-[#89DDFF]">(</span><span class="dark:text-[#EEFFFF] text-[#000000]">x</span><span class="dark:text-[#89DDFF]">[</span><span class="dark:text-[#EEFFFF] text-[#000000]">i</span><span class="dark:text-[#89DDFF]">]</span><span class="dark:text-[#89DDFF] text-[#000000]">-</span><span class="dark:text-[#EEFFFF] text-[#000000]">mu</span><span class="dark:text-[#89DDFF]">)</span><span class="dark:text-[#89DDFF] text-[#000000]">*</span><span class="dark:text-[#89DDFF]">(</span><span class="dark:text-[#EEFFFF] text-[#000000]">x</span><span class="dark:text-[#89DDFF]">[</span><span class="dark:text-[#EEFFFF] text-[#000000]">i</span><span class="dark:text-[#89DDFF]">]</span><span class="dark:text-[#89DDFF] text-[#000000]">-</span><span class="dark:text-[#EEFFFF] text-[#000000]">mu</span><span class="dark:text-[#89DDFF]">)</span> 
</span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]">10</span><span class="cl">    <span class="dark:text-[#EEFFFF] text-[#000000]">var</span><span class="dark:text-[#89DDFF] text-[#000000]">=</span><span class="dark:text-[#EEFFFF] text-[#000000]">var</span><span class="dark:text-[#89DDFF] text-[#000000]">/</span><span class="dark:text-[#EEFFFF] text-[#000000]">x</span><span class="dark:text-[#89DDFF] text-[#000000]">.</span><span class="dark:text-[#EEFFFF] text-[#000000]">shape</span><span class="dark:text-[#89DDFF]">[</span><span class="dark:text-[#F78C6C] text-[#1c01ce]">0</span><span class="dark:text-[#89DDFF]">]</span>
</span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]">11</span><span class="cl">    <span class="dark:text-[#EEFFFF] text-[#000000]">e</span><span class="dark:text-[#89DDFF] text-[#000000]">=</span><span class="dark:text-[#F78C6C] text-[#1c01ce]">0.000000001</span>
</span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]">12</span><span class="cl">    <span class="dark:text-[#BB80B3] text-[#a90d91]">return</span> <span class="dark:text-[#89DDFF]">(</span><span class="dark:text-[#EEFFFF] text-[#000000]">x</span><span class="dark:text-[#89DDFF] text-[#000000]">-</span><span class="dark:text-[#EEFFFF] text-[#000000]">mu</span><span class="dark:text-[#89DDFF]">)</span><span class="dark:text-[#89DDFF] text-[#000000]">/</span><span class="dark:text-[#89DDFF]">(</span><span class="dark:text-[#EEFFFF] text-[#000000]">np</span><span class="dark:text-[#89DDFF] text-[#000000]">.</span><span class="dark:text-[#EEFFFF] text-[#000000]">sqrt</span><span class="dark:text-[#89DDFF]">(</span><span class="dark:text-[#EEFFFF] text-[#000000]">var</span><span class="dark:text-[#89DDFF]">)</span><span class="dark:text-[#89DDFF] text-[#000000]">+</span><span class="dark:text-[#EEFFFF] text-[#000000]">e</span><span class="dark:text-[#89DDFF]">)</span>
</span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]">13</span><span class="cl"><span class="dark:text-[#EEFFFF] text-[#000000]">batchnorm</span><span class="dark:text-[#89DDFF]">(</span><span class="dark:text-[#EEFFFF] text-[#000000]">x</span><span class="dark:text-[#89DDFF]">)</span><span class="dark:text-[#89DDFF] text-[#000000]">.</span><span class="dark:text-[#EEFFFF] text-[#000000]">shape</span>
</span></span></code></pre></div></div><pre><code>(10, 4)
</code></pre>
<h3 class='text-xl  font-extrabold	text-slate-900 tracking-tight dark:text-slate-200  py-1.5' id="layer-norm">layer norm</h3>
<!-- raw HTML omitted -->
<div class="relative z-10 mx-2 my-6 col-span-3 dark:bg-slate-800 bg-white font-semibold rounded-md shadow-lg  ring-1 ring-black/10 dark:ring-1 dark:ring-white/10 dark:ring-inset">
				<div class="relative flex text-slate-400 text-sm leading-6">
						<div class="mt-2 flex-none dark:text-sky-300 text-slate-800 border-t border-b border-t-transparent border-b-slate-600 dark:border-b-sky-300 px-4 py-1 flex items-center">
								language:$2
						</div>
						<div class="flex-auto flex pt-2 rounded-tr-xl overflow-hidden">
								<div class="flex-auto -mr-px bg-slate-100 dark:bg-slate-700/50 border border-slate-500/30 rounded-tl">
								</div>
						</div>
						<div class="absolute top-2 right-0 h-8 flex items-center pr-4">
								<div class="relative flex -mr-2">
								<span class="copycontent"/>
								</div>
						</div>
				</div>
	<div class="highlight p-4  text-sm  overflow-x-auto  text-slate-800 dark:text-slate-200
							scrollbar-thin  scrollbar-thumb-rounded-md scrollbar-track-rounded-md
							scrollbar-track-white scrollbar-thumb-slate-200 
							dark:scrollbar-track-slate-800 dark:scrollbar-thumb-slate-500
	"><pre><code><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]"> 1</span><span class="cl"><span class="dark:text-[#BB80B3] text-[#a90d91]">def</span> <span class="dark:text-[#82AAFF] text-[#000000]">layernorm</span><span class="dark:text-[#89DDFF]">(</span><span class="dark:text-[#EEFFFF] text-[#000000]">x</span><span class="dark:text-[#89DDFF]">):</span>
</span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]"> 2</span><span class="cl">    <span class="dark:text-[#C3E88D] text-[#c41a16]">&#34;&#34;&#34;
</span></span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]"> 3</span><span class="cl"><span class="dark:text-[#C3E88D] text-[#c41a16]">    x.shape:b,dim
</span></span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]"> 4</span><span class="cl"><span class="dark:text-[#C3E88D] text-[#c41a16]">    &#34;&#34;&#34;</span>
</span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]"> 5</span><span class="cl">    <span class="dark:text-[#546E7A] text-[#177500]">####维度交换</span>
</span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]"> 6</span><span class="cl">    <span class="dark:text-[#EEFFFF] text-[#000000]">x</span><span class="dark:text-[#89DDFF] text-[#000000]">=</span><span class="dark:text-[#EEFFFF] text-[#000000]">x</span><span class="dark:text-[#89DDFF] text-[#000000]">.</span><span class="dark:text-[#EEFFFF] text-[#000000]">transpose</span><span class="dark:text-[#89DDFF]">(</span><span class="dark:text-[#F78C6C] text-[#1c01ce]">1</span><span class="dark:text-[#89DDFF]">,</span><span class="dark:text-[#F78C6C] text-[#1c01ce]">0</span><span class="dark:text-[#89DDFF]">)</span>
</span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]"> 7</span><span class="cl">    <span class="dark:text-[#EEFFFF] text-[#000000]">mu</span><span class="dark:text-[#89DDFF] text-[#000000]">=</span><span class="dark:text-[#EEFFFF] text-[#000000]">np</span><span class="dark:text-[#89DDFF] text-[#000000]">.</span><span class="dark:text-[#EEFFFF] text-[#000000]">sum</span><span class="dark:text-[#89DDFF]">(</span><span class="dark:text-[#EEFFFF] text-[#000000]">x</span><span class="dark:text-[#89DDFF]">,</span><span class="dark:text-[#F78C6C] text-[#1c01ce]">0</span><span class="dark:text-[#89DDFF]">)</span><span class="dark:text-[#89DDFF] text-[#000000]">/</span><span class="dark:text-[#EEFFFF] text-[#000000]">x</span><span class="dark:text-[#89DDFF] text-[#000000]">.</span><span class="dark:text-[#EEFFFF] text-[#000000]">shape</span><span class="dark:text-[#89DDFF]">[</span><span class="dark:text-[#F78C6C] text-[#1c01ce]">0</span><span class="dark:text-[#89DDFF]">]</span>
</span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]"> 8</span><span class="cl">
</span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]"> 9</span><span class="cl">    <span class="dark:text-[#EEFFFF] text-[#000000]">var</span><span class="dark:text-[#89DDFF] text-[#000000]">=</span><span class="dark:text-[#F78C6C] text-[#1c01ce]">0</span>
</span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]">10</span><span class="cl">    <span class="dark:text-[#BB80B3] text-[#a90d91]">for</span> <span class="dark:text-[#EEFFFF] text-[#000000]">i</span> <span class="ow">in</span> <span class="dark:text-[#82AAFF] text-[#a90d91]">range</span><span class="dark:text-[#89DDFF]">(</span><span class="dark:text-[#82AAFF] text-[#a90d91]">len</span><span class="dark:text-[#89DDFF]">(</span><span class="dark:text-[#EEFFFF] text-[#000000]">x</span><span class="dark:text-[#89DDFF]">)):</span>
</span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]">11</span><span class="cl">        <span class="dark:text-[#EEFFFF] text-[#000000]">var</span><span class="dark:text-[#89DDFF] text-[#000000]">+=</span><span class="dark:text-[#89DDFF]">(</span><span class="dark:text-[#EEFFFF] text-[#000000]">x</span><span class="dark:text-[#89DDFF]">[</span><span class="dark:text-[#EEFFFF] text-[#000000]">i</span><span class="dark:text-[#89DDFF]">]</span><span class="dark:text-[#89DDFF] text-[#000000]">-</span><span class="dark:text-[#EEFFFF] text-[#000000]">mu</span><span class="dark:text-[#89DDFF]">)</span><span class="dark:text-[#89DDFF] text-[#000000]">*</span><span class="dark:text-[#89DDFF]">(</span><span class="dark:text-[#EEFFFF] text-[#000000]">x</span><span class="dark:text-[#89DDFF]">[</span><span class="dark:text-[#EEFFFF] text-[#000000]">i</span><span class="dark:text-[#89DDFF]">]</span><span class="dark:text-[#89DDFF] text-[#000000]">-</span><span class="dark:text-[#EEFFFF] text-[#000000]">mu</span><span class="dark:text-[#89DDFF]">)</span> 
</span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]">12</span><span class="cl">    <span class="dark:text-[#EEFFFF] text-[#000000]">var</span><span class="dark:text-[#89DDFF] text-[#000000]">=</span><span class="dark:text-[#EEFFFF] text-[#000000]">var</span><span class="dark:text-[#89DDFF] text-[#000000]">/</span><span class="dark:text-[#EEFFFF] text-[#000000]">x</span><span class="dark:text-[#89DDFF] text-[#000000]">.</span><span class="dark:text-[#EEFFFF] text-[#000000]">shape</span><span class="dark:text-[#89DDFF]">[</span><span class="dark:text-[#F78C6C] text-[#1c01ce]">0</span><span class="dark:text-[#89DDFF]">]</span>
</span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]">13</span><span class="cl">    <span class="dark:text-[#EEFFFF] text-[#000000]">e</span><span class="dark:text-[#89DDFF] text-[#000000]">=</span><span class="dark:text-[#F78C6C] text-[#1c01ce]">0.000000001</span>
</span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]">14</span><span class="cl">    <span class="dark:text-[#BB80B3] text-[#a90d91]">return</span> <span class="dark:text-[#89DDFF]">(</span><span class="dark:text-[#EEFFFF] text-[#000000]">x</span><span class="dark:text-[#89DDFF] text-[#000000]">-</span><span class="dark:text-[#EEFFFF] text-[#000000]">mu</span><span class="dark:text-[#89DDFF]">)</span><span class="dark:text-[#89DDFF] text-[#000000]">/</span><span class="dark:text-[#89DDFF]">(</span><span class="dark:text-[#EEFFFF] text-[#000000]">np</span><span class="dark:text-[#89DDFF] text-[#000000]">.</span><span class="dark:text-[#EEFFFF] text-[#000000]">sqrt</span><span class="dark:text-[#89DDFF]">(</span><span class="dark:text-[#EEFFFF] text-[#000000]">var</span><span class="dark:text-[#89DDFF]">)</span><span class="dark:text-[#89DDFF] text-[#000000]">+</span><span class="dark:text-[#EEFFFF] text-[#000000]">e</span><span class="dark:text-[#89DDFF]">)</span>
</span></span><span class="flex"><span class="whitespace-pre select-none mr-1.5 px-1.5 text-[#7f7f7f]">15</span><span class="cl"><span class="dark:text-[#EEFFFF] text-[#000000]">layernorm</span><span class="dark:text-[#89DDFF]">(</span><span class="dark:text-[#EEFFFF] text-[#000000]">x</span><span class="dark:text-[#89DDFF]">)</span><span class="dark:text-[#89DDFF] text-[#000000]">.</span><span class="dark:text-[#EEFFFF] text-[#000000]">shape</span>
</span></span></code></pre></div></div><pre><code>(4, 10)
</code></pre>
<h3 class='text-xl  font-extrabold	text-slate-900 tracking-tight dark:text-slate-200  py-1.5' id="heading">正则化</h3>
<p class='my-2 indent-8 text-base  text-slate-900 dark:text-slate-200'>L1,L2，
正则化:<span class="math inline">\(loss=(lossold+a*||w||^2_2)\)</span>
考虑梯度更新:dw=dw_old+a<em>2</em>w;
更新时w_new=w_old-k*dw</p>
<p class='my-2 indent-8 text-base  text-slate-900 dark:text-slate-200'><img src="./assets/regu.png" alt="Figure [robot]: Gelu">
为啥 正则化降低过拟合(高方差),考虑线性拟合，正则化对权重做了限制（比如取值0，网络结构缩小，高级拟合变低级拟合），防止过学习细节。</p>
<p class='my-2 indent-8 text-base  text-slate-900 dark:text-slate-200'>考虑正则化项的权重参数a:
考虑tanh 激活函数， a设置过大，会导致w小，tanh的取值是接近线性部分，也就是没有激活函数，也就是只有一层线性层，欠拟合。；a设置的小，w取得大，tanh接近0，也就是没有激活函数，也就是只有一层线性层，欠拟合。</p>
<h2 class='text-2xl font-extrabold text-slate-900 tracking-tight dark:text-slate-200  py-2' id="2---l2-regularization">2 - L2 Regularization</h2>
<p class='my-2 indent-8 text-base  text-slate-900 dark:text-slate-200'>The standard way to avoid overfitting is called <strong>L2 regularization</strong>. It consists of appropriately modifying your cost function, from:</p>
<p><span class="math display">\[To:
$$J_{regularized} = \small \underbrace{-\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} }_\text{cross-entropy cost} + \underbrace{\frac{1}{m} \frac{\lambda}{2} \sum\limits_l\sum\limits_k\sum\limits_j W_{k,j}^{[l]2} }_\text{L2 regularization cost} \tag{2}$$

Let's modify your cost and observe the consequences.

**Exercise**: Implement `compute_cost_with_regularization()` which computes the cost given by formula (2). To calculate $\sum\limits_k\sum\limits_j W_{k,j}^{[l]2}$  , use :
```python
np.sum(np.square(Wl))
```
Note that you have to do this for $W^{[1]}$, $W^{[2]}$ and $W^{[3]}$, then sum the three terms and multiply by $ \frac{1}{m} \frac{\lambda}{2} $.

### dropout
防止过拟合的原因；可以认为dropout 使得模型结构每次都不一样；类似于baging

训练时采用，测试时不采用；
前向传播，使得一部分神经元失活D，反向传播同样da *D;D是0,1矩阵

## 3 - Dropout

Finally, **dropout** is a widely used regularization technique that is specific to deep learning. 
**It randomly shuts down some neurons in each iteration.** Watch these two videos to see what this means!

<!--
To understand drop-out, consider this conversation with a friend:
- Friend: "Why do you need all these neurons to train your network and classify images?". 
- You: "Because each neuron contains a weight and can learn specific features/details/shape of an image. The more neurons I have, the more featurse my model learns!"
- Friend: "I see, but are you sure that your neurons are learning different features and not all the same features?"
- You: "Good point... Neurons in the same layer actually don't talk to each other. It should be definitly possible that they learn the same image features/shapes/forms/details... which would be redundant. There should be a solution."
!--> 


<center>
<video width="620" height="440" src="images/dropout1_kiank.mp4" type="video/mp4" controls>
</video>
</center>
<br>
<caption><center> <u> Figure 2 </u>: Drop-out on the second hidden layer. <br> At each iteration, you shut down (= set to zero) each neuron of a layer with probability $1 - keep\_prob$ or keep it with probability $keep\_prob$ (50% here). The dropped neurons don't contribute to the training in both the forward and backward propagations of the iteration. </center></caption>

<center>
<video width="620" height="440" src="images/dropout2_kiank.mp4" type="video/mp4" controls>
</video>
</center>

<caption><center> <u> Figure 3 </u>: Drop-out on the first and third hidden layers. <br> $1^{st}$ layer: we shut down on average 40% of the neurons.  $3^{rd}$ layer: we shut down on average 20% of the neurons. </center></caption>


When you shut some neurons down, you actually modify your model. The idea behind drop-out is that at each iteration, you train a different model that uses only a subset of your neurons. With dropout, your neurons thus become less sensitive to the activation of one other specific neuron, because that other neuron might be shut down at any time. 

### 3.1 - Forward propagation with dropout

**Exercise**: Implement the forward propagation with dropout. You are using a 3 layer neural network, and will add dropout to the first and second hidden layers. We will not apply dropout to the input layer or output layer. 

**Instructions**:
You would like to shut down some neurons in the first and second layers. To do that, you are going to carry out 4 Steps:
1. In lecture, we dicussed creating a variable $d^{[1]}$ with the same shape as $a^{[1]}$ using `np.random.rand()` to randomly get numbers between 0 and 1. Here, you will use a vectorized implementation, so create a random matrix $D^{[1]} = [d^{[1](1)} d^{[1](2)} ... d^{[1](m)}] $ of the same dimension as $A^{[1]}$.
2. Set each entry of $D^{[1]}$ to be 1 with probability (`keep_prob`), and 0 otherwise.

**Hint:** Let's say that keep_prob = 0.8, which means that we want to keep about 80% of the neurons and drop out about 20% of them.  We want to generate a vector that has 1's and 0's, where about 80% of them are 1 and about 20% are 0.
This python statement:  
`X = (X < keep_prob).astype(int)`  

is conceptually the same as this if-else statement (for the simple case of a one-dimensional array) :

```
for i,v in enumerate(x):
    if v < keep_prob:
        x[i] = 1
    else: # v >= keep_prob
        x[i] = 0
```
Note that the `X = (X < keep_prob).astype(int)` works with multi-dimensional arrays, and the resulting output preserves the dimensions of the input array.

Also note that without using `.astype(int)`, the result is an array of booleans `True` and `False`, which Python automatically converts to 1 and 0 if we multiply it with numbers.  (However, it's better practice to convert data into the data type that we intend, so try using `.astype(int)`.)

3. Set $A^{[1]}$ to $A^{[1]} * D^{[1]}$. (You are shutting down some neurons). You can think of $D^{[1]}$ as a mask, so that when it is multiplied with another matrix, it shuts down some of the values.
4. Divide $A^{[1]}$ by `keep_prob`. By doing this you are assuring that the result of the cost will still have the same expected value as without drop-out. (This technique is also called inverted dropout.)

### 3.2 - Backward propagation with dropout

**Exercise**: Implement the backward propagation with dropout. As before, you are training a 3 layer network. Add dropout to the first and second hidden layers, using the masks $D^{[1]}$ and $D^{[2]}$ stored in the cache. 

**Instruction**:
Backpropagation with dropout is actually quite easy. You will have to carry out 2 Steps:
1. You had previously shut down some neurons during forward propagation, by applying a mask $D^{[1]}$ to `A1`. In backpropagation, you will have to shut down the same neurons, by reapplying the same mask $D^{[1]}$ to `dA1`. 
2. During forward propagation, you had divided `A1` by `keep_prob`. In backpropagation, you'll therefore have to divide `dA1` by `keep_prob` again (the calculus interpretation is that if $A^{[1]}$ is scaled by `keep_prob`, then its derivative $dA^{[1]}$ is also scaled by the same `keep_prob`).



```python

```
\]</span></p>
